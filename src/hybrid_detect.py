import cv2
import mediapipe as mp
import numpy as np
import torch
import collections
import time
import subprocess  # ì†Œë¦¬/ì „í™” ê¸°ëŠ¥ìš© (íŒ€ì› ì½”ë“œ ì•„ì´ë””ì–´)
import pickle

# ê¸°ì¡´ ìš°ë¦¬ ëª¨ë“ˆ (ê²½ë¡œ í™•ì¸ í•„ìš”)
from utils_features import FeatureExtractor
from model_definition import GRUAutoencoder, SimpleGRUAE

# =========================
# CONFIG & STATE DEFINITIONS
# =========================
SEQ_LEN = 30
MODEL_PATH = "data/models/gru_ae.pth"
SCALER_PATH = "data/models/scaler.pkl"
THRESHOLD_PATH = "data/models/threshold.txt"

# FSM States (íŒ€ì› ì½”ë“œ ì°¨ìš©)
NORMAL = "NORMAL"
SUSPECT = "SUSPECT"     # ì˜ì‹¬ (ê²½ê³ ìŒ) - "ê´œì°®ìœ¼ì„¸ìš”?"
EMERGENCY = "EMERGENCY" # ì‘ê¸‰ (ì „í™”/ì‹ ê³ ) - 10ì´ˆ ì´ìƒ ì§€ì† ì‹œ
RECOVERY = "RECOVERY"

class HybridDMS:
    def __init__(self):
        # 1. Load GRU Model (The Brain)
        self.device = torch.device('cpu')
        try:
            self.scaler = pickle.load(open(SCALER_PATH, "rb"))
            with open(THRESHOLD_PATH, "r") as f:
                self.gru_threshold = float(f.read())
            
            # ëª¨ë¸ êµ¬ì¡°ëŠ” í•™ìŠµ ë•Œì™€ ê°™ì•„ì•¼ í•¨ (Hidden dim ë“±)
            self.model = SimpleGRUAE(input_dim=2, hidden_dim=16) 
            self.model.load_state_dict(torch.load(MODEL_PATH, map_location=self.device))
            self.model.eval()
            print("[System] GRU Model Loaded Successfully.")
        except Exception as e:
            print(f"[Error] ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ (í•™ìŠµ ë¨¼ì € í•˜ì„¸ìš”): {e}")
            self.gru_threshold = 0.5 # Default fallback

        self.extractor = FeatureExtractor()
        
        # Buffers
        self.feature_buffer = collections.deque(maxlen=SEQ_LEN) # For GRU
        self.raw_pitch_buffer = collections.deque(maxlen=60)    # For FFT (2 seconds @ 30fps)

        # FSM Variables
        self.state = NORMAL
        self.suspect_start_time = None
        self.emergency_start_time = None
        
        self.last_audio_time = 0
        self.call_triggered = False

    def get_fft_energy(self):
        """
        FFTë¥¼ ì‚¬ìš©í•´ ìµœê·¼ ê³ ê°œ ì›€ì§ì„ì˜ 'ì—ë„ˆì§€'ì™€ 'ì£¼íŒŒìˆ˜ íŒ¨í„´'ì„ ë¶„ì„
        """
        if len(self.raw_pitch_buffer) < 30:
            return 0.0, 0.0 # Not enough data

        data = np.array(self.raw_pitch_buffer)
        
        # DC ì„±ë¶„(í‰ê· ) ì œê±° -> ì›€ì§ì„ì˜ ë³€í™”ëŸ‰ë§Œ ë³´ê¸° ìœ„í•´
        data_centered = data - np.mean(data)
        
        # FFT ìˆ˜í–‰
        fft_vals = np.fft.fft(data_centered)
        fft_freq = np.fft.fftfreq(len(data_centered))
        
        # Power Spectrum (ì—ë„ˆì§€)
        power = np.abs(fft_vals) ** 2
        
        # 1. Total Motion Energy (ì „ì²´ ì›€ì§ì„ ì—ë„ˆì§€)
        total_energy = np.sum(power)
        
        # 2. High Frequency Energy (ë–¨ë¦¼/ê²½ë ¨ ê°ì§€ìš©, 5Hz ì´ìƒ ëŒ€ì—­)
        # 30fps ê¸°ì¤€, ì¸ë±ìŠ¤ ì ˆë°˜ì´ Nyquist frequency(15Hz)
        # ëŒ€ëµì ì¸ ì¸ë±ìŠ¤ë¡œ ê³ ì£¼íŒŒ ëŒ€ì—­ í•„í„°ë§
        idx_high = int(len(power) * 0.3) 
        high_freq_energy = np.sum(power[idx_high:])
        
        return total_energy, high_freq_energy

    def update(self, image):
        # -----------------------------
        # 1. Feature Extraction (Our 3D PnP)
        # -----------------------------
        img_h, img_w, _ = image.shape
        # (ë©”ì¸ ë£¨í”„ì—ì„œ ë¯¸ë””ì–´íŒŒì´í”„ ì²˜ë¦¬ëŠ” ì´ë¯¸ì§€ ë„˜ê²¨ë°›ê¸° ì „ ìˆ˜í–‰ë¨ì„ ê°€ì •í•˜ê±°ë‚˜ ì—¬ê¸°ì„œ ìˆ˜í–‰)
        # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì™¸ë¶€ì—ì„œ landmarksë¥¼ ë„˜ê²¨ë°›ëŠ” êµ¬ì¡° ëŒ€ì‹ , 
        # FeatureExtractorê°€ landmarksë¥¼ ë°›ëŠ”ë‹¤ê³  ê°€ì •í•˜ê³  ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§ì€ mainì— ë‘¡ë‹ˆë‹¤.
        pass 

    def process_frame(self, ear, pitch):
        # Data Buffering
        self.raw_pitch_buffer.append(pitch)
        
        # GRU Preprocessing
        scaled = self.scaler.transform([[ear, pitch]])[0]
        self.feature_buffer.append(scaled)

        # -----------------------------
        # 2. Hybrid Logic (GRU + FFT)
        # -----------------------------
        gru_anomaly_score = 0.0
        
        # A) GRU Anomaly Score
        if len(self.feature_buffer) == SEQ_LEN:
            seq_data = np.array(self.feature_buffer)
            input_tensor = torch.FloatTensor(seq_data).unsqueeze(0)
            with torch.no_grad():
                recon = self.model(input_tensor)
            gru_anomaly_score = torch.mean((input_tensor - recon) ** 2).item()

        # B) FFT Analysis (ì˜ì‹ ì†Œì‹¤ ê²€ì¦)
        motion_energy, tremor_energy = self.get_fft_energy()
        
        # --- JUDGEMENT LOGIC ---
        # ì¡°ê±´ 1: GRUê°€ "í‰ì†Œì™€ ë‹¤ë¥´ë‹¤"ê³  íŒë‹¨ (Threshold ì´ˆê³¼)
        is_abnormal_pattern = gru_anomaly_score > self.gru_threshold
        
        # ì¡°ê±´ 2: ëˆˆì„ ê°ê³  ìˆìŒ (EAR < 0.22)
        is_eyes_closed = ear < 0.22
        
        # ì¡°ê±´ 3: ì›€ì§ì„ì´ ê·¹ë„ë¡œ ì—†ê±°ë‚˜(ê¸°ì ˆ) OR ë„ˆë¬´ ì‹¬í•¨(ë°œì‘) - FFT í™œìš©
        # ì—ë„ˆì§€ 500 ë¯¸ë§Œì´ë©´ "ì¶• ëŠ˜ì–´ì§", 10000 ì´ìƒì´ë©´ "ë°œì‘" (ì„ì˜ ê°’, íŠœë‹ í•„ìš”)
        is_motionless = motion_energy < 50.0 
        is_seizure = tremor_energy > 5000.0

        # ì¢…í•© íŒë‹¨: "ë¹„ì •ìƒ íŒ¨í„´" AND ("ëˆˆê°ìŒ" OR "ì›€ì§ì„ì´ìƒ")
        is_danger = is_abnormal_pattern and (is_eyes_closed or is_motionless or is_seizure)

        return gru_anomaly_score, motion_energy, is_danger

    def update_fsm(self, is_danger, now):
        """ ìƒíƒœ ì²œì´ (Finite State Machine) """
        
        if self.state == NORMAL:
            if is_danger:
                self.state = SUSPECT
                self.suspect_start_time = now
        
        elif self.state == SUSPECT:
            if not is_danger:
                # íšŒë³µë¨ -> ë‹¤ì‹œ NORMAL
                self.state = NORMAL
                self.suspect_start_time = None
            else:
                # ìœ„í—˜ ì§€ì† ì‹œê°„ ì²´í¬ (ì˜ˆ: 3ì´ˆ ì´ìƒ ì§€ì† ì‹œ ì‘ê¸‰)
                if (now - self.suspect_start_time) > 3.0:
                    self.state = EMERGENCY
                    self.emergency_start_time = now
                    self.call_triggered = False

        elif self.state == EMERGENCY:
            if not is_danger:
                # íšŒë³µ ëª¨ë“œë¡œ ì „í™˜
                self.state = RECOVERY
            else:
                # ì‘ê¸‰ ì¡°ì¹˜ (ì „í™” ê±¸ê¸° ë“±)
                self.trigger_emergency_action(now)

        elif self.state == RECOVERY:
            if is_danger:
                self.state = EMERGENCY # ì¬ë°œ
            else:
                # ì¼ì • ì‹œê°„ ì•ˆì •ë˜ë©´ Normalë¡œ
                # (ê°„ë‹¨í•˜ê²Œ ë°”ë¡œ Normalë¡œ ê°€ê±°ë‚˜ íƒ€ì´ë¨¸ ë‘˜ ìˆ˜ ìˆìŒ)
                self.state = NORMAL

        return self.state

    def trigger_emergency_action(self, now):
        """ ì‹¤ì œ ì•Œë¦¼/ì „í™” ë¡œì§ (Mac/Windows í˜¸í™˜ì„ ìœ„í•´ printë¡œ ëŒ€ì²´í•˜ê±°ë‚˜ subprocess ì‚¬ìš©) """
        if not self.call_triggered:
            # 1. ê²½ê³ ìŒ ë¹¡ì„¸ê²Œ
            if (now - self.last_audio_time) > 1.0:
                print("\a") # ìœˆë„ìš° ë¹„í”„ìŒ
                # Mac ì˜ˆì‹œ: subprocess.Popen(["afplay", "/System/Library/Sounds/Ping.aiff"])
                self.last_audio_time = now
            
            # 2. 10ì´ˆ ì§€ë‚˜ë©´ 119/ì§€ì¸ í˜¸ì¶œ
            if (now - self.emergency_start_time) > 10.0:
                print(">>> ğŸš¨ EMERGENCY CALL ACTIVATED! calling 119... <<<")
                self.call_triggered = True

def main():
    # Setup MediaPipe
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(max_num_faces=1, refine_landmarks=True)
    
    detector = HybridDMS()
    cap = cv2.VideoCapture(0)
    
    print("=== Hybrid DMS System Started ===")
    print("Logic: GRU(Brain) + FFT(Frequency) + FSM(State)")
    
    while cap.isOpened():
        success, image = cap.read()
        if not success: continue
        
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_mesh.process(image_rgb)
        
        current_time = time.time()
        
        ear = 0.3 # Default (Safe)
        pitch = 0.0
        
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                # Feature Extraction
                ear = detector.extractor.calculate_ear(face_landmarks.landmark)
                pitch = detector.extractor.calculate_head_pose_pitch(face_landmarks.landmark, image.shape)
                
                # Mesh Visualization
                mp.solutions.drawing_utils.draw_landmarks(
                    image=image, landmark_list=face_landmarks,
                    connections=mp_face_mesh.FACEMESH_TESSELATION,
                    connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(100,100,100), thickness=1))

        # --- CORE PROCESS ---
        gru_score, fft_energy, is_danger = detector.process_frame(ear, pitch)
        current_state = detector.update_fsm(is_danger, current_time)
        
        # --- VISUALIZATION ---
        
        # ìƒíƒœë³„ ìƒ‰ìƒ
        color = (0, 255, 0) # Normal (Green)
        if current_state == SUSPECT: color = (0, 255, 255) # Yellow
        elif current_state == EMERGENCY: color = (0, 0, 255) # Red
        
        # 1. State Box
        cv2.rectangle(image, (0, 0), (640, 80), (0, 0, 0), -1)
        cv2.putText(image, f"STATE: {current_state}", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
        
        # 2. Info String
        info = f"GRU:{gru_score:.4f} | FFT:{fft_energy:.1f} | Pitch:{pitch:.0f}"
        cv2.putText(image, info, (20, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # 3. Warning Message
        if current_state == EMERGENCY:
            cv2.putText(image, "EMERGENCY! UNCONSCIOUS!", (50, 300), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 4)

        cv2.imshow("Hybrid DMS", image)
        if cv2.waitKey(5) & 0xFF == ord('q'):
            break
            
    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
